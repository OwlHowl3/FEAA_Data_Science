{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a723nZnQdyJI","colab_type":"text"},"source":["# Titanic dataset again. Yay or nay? "]},{"cell_type":"code","metadata":{"id":"S2kS-_Old46E","colab_type":"code","colab":{}},"source":["# take the titanic dataset and apply the preprocessing techniques you used in the previous labs\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-qnyownPeGz1","colab_type":"code","colab":{}},"source":["# apply decision tree with different parameters (entopy or gini, max depth, min_samples_leaf etc. You can find them in the sklearn docs )\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_e-x_yVeZKf","colab_type":"code","colab":{}},"source":["# apply random forest with different parameters (n_estimators, bootstrap, max_features etc. You can find them in the sklearn docs )\n"],"execution_count":0,"outputs":[]}]}